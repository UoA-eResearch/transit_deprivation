{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load location index\n",
    "with open(\"data/akl_loc_idx.pkl\", 'rb') as f:\n",
    "    loc_idx = pickle.load(f) # datazone to point index\n",
    "    idx_loc = {v:k for k, v in loc_idx.items()} # point index to datazone    \n",
    "    print(f\" -- loaded location index with dimension {len(loc_idx)}\")\n",
    "\n",
    "# load time index\n",
    "with open(\"data/akl_t_idx.pkl\", 'rb') as f:\n",
    "    t_idx = pickle.load(f)\n",
    "    print(f\" -- loaded time index with dimension {len(t_idx)}\")\n",
    "    \n",
    "# load precomputed odt\n",
    "with open(\"data/akl_odt.npy\", 'rb') as f:\n",
    "    odt = np.load(f)\n",
    "    print(f\" -- loaded odt cube with dimensions {odt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/akl_polygons.geojson\", 'r') as f:\n",
    "    polys = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add id field to each feature (requried by plotly even though it's supposed to work via subproperty ref.)\n",
    "for f in polys[\"features\"]:\n",
    "    f[\"id\"] = f[\"properties\"][\"DZ2018\"]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer necessary, we're keeping all properties\n",
    "\n",
    "# # remove properties DZ2018 as it is now duplicated\n",
    "# for f in polys[\"features\"]:\n",
    "#     f[\"properties\"] = f[\"properties\"].pop(\"DZ2018\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine polygons with duplicate feature ids\n",
    "features = {}\n",
    "for f in polys[\"features\"]:\n",
    "    fid = f[\"id\"]\n",
    "    if fid not in features:\n",
    "        features[fid] = f        \n",
    "    else:\n",
    "        f_poly = f[\"geometry\"][\"coordinates\"]\n",
    "        features[fid][\"geometry\"][\"coordinates\"] += f_poly    \n",
    "        #print(fid, len(features[fid][\"geometry\"][\"coordinates\"]))\n",
    "polys[\"features\"] = list(features.values())\n",
    "print(f\"total features: {len(polys['features'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter polygons by those that have population centroids\n",
    "pids = [k for k, v in loc_idx.items()]\n",
    "valid_features = [f for f in polys[\"features\"] if f[\"id\"] in pids]\n",
    "polys[\"features\"] = valid_features\n",
    "print(\"total features:\", len(polys[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was an attempt to reduce the number of polygons displayed by filtering out those that have on valid joureny data\n",
    "# it is no longer necessary becaue deck.gl is performant\n",
    "\n",
    "filter_valid = False\n",
    "\n",
    "if filter_valid:\n",
    "    # filter polygons by those that have valid journeys\n",
    "    jsum = np.sum(np.nansum(odt, axis=2), axis=0)\n",
    "    valid_idx = [i for i, v in enumerate(jsum) if v > 0]\n",
    "    valid_features = [f for f in polys[\"features\"] if loc_idx[f[\"id\"]] in valid_idx]\n",
    "    polys[\"features\"] = valid_features\n",
    "    print(f\"total features: {len(polys['features'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cube indices to data folder used by the web frontend\n",
    "with open(\"data/../frontend/akl/akl_loc_idx.json\", \"w\") as f:\n",
    "    json.dump(loc_idx, f)\n",
    "with open(\"data/../frontend/akl/akl_idx_loc.json\", \"w\") as f:\n",
    "    json.dump(idx_loc, f) \n",
    "\n",
    "# write time index in milliseconds\n",
    "idx_t = {v:int(k.timestamp())*1000 for k,v in t_idx.items()}\n",
    "with open(\"data/../frontend/akl/akl_idx_t.json\", \"w\") as f:\n",
    "    json.dump(idx_t, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the cleaned polygon data to the frontend folder\n",
    "# note the .json extension to avoid an additional webpack rule\n",
    "with open(\"data/../frontend/akl/akl_polygons_id.json\", \"w\") as f:\n",
    "    json.dump(polys, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan values in odt\n",
    "nan_value = -1\n",
    "odt_nan = np.nan_to_num(odt, nan=nan_value)\n",
    "fmt = np.vectorize(lambda x: f\"{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outbound cube slices to the backend data dir\n",
    "\n",
    "target_dir = \"data/../backend/outbound/akl\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# save a separate dt_slice file for each location\n",
    "for f in polys[\"features\"]:\n",
    "    # origin id and odt index of origin\n",
    "    o = f[\"id\"]           \n",
    "    o_idx = loc_idx[o]\n",
    "    # get dt slice for the origin\n",
    "    dt_slice = odt_nan[o_idx, :, :]            \n",
    "    dt_slice = fmt(dt_slice).astype(np.float).tolist()\n",
    "    # save the slice\n",
    "    path = os.path.join(target_dir, f\"{o}.json\")\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(dt_slice, f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inbound cube slices to the backend data dir\n",
    "\n",
    "target_dir = \"data/../backend/inbound/akl\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# save a separate dt_slice file for each location\n",
    "for f in polys[\"features\"]:\n",
    "    # destination id and odt index of destination\n",
    "    d = f[\"id\"]           \n",
    "    d_idx = loc_idx[d]\n",
    "    # get ot slice for the destination\n",
    "    ot_slice = odt_nan[:, d_idx, :]            \n",
    "    ot_slice = fmt(ot_slice).astype(np.float).tolist()\n",
    "    # save the slice\n",
    "    path = os.path.join(target_dir, f\"{d}.json\")\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(ot_slice, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data needs to be wgs84\n",
    "locations = [f[\"id\"] for f in polys[\"features\"]]\n",
    "fig = px.choropleth_mapbox(\n",
    "        geojson=polys, \n",
    "        featureidkey=\"id\",\n",
    "        locations=locations,        \n",
    "        center = {\"lat\": -36.8485, \"lon\": 174.7633},\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        zoom=12)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
